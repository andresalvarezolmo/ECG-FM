{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "root = '/home/aa2650/playground/ECG-FM'\n",
    "FAIRSEQ_SIGNALS_ROOT = '/home/aa2650/playground/fairseq-signals'\n",
    "FAIRSEQ_SIGNALS_ROOT = FAIRSEQ_SIGNALS_ROOT.rstrip('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = 80\n",
    "# validation = 10\n",
    "# test = 10\n",
    "# data_split = f\"{train}-{validation}-{test}\"\n",
    "# experiment_run = 5\n",
    "\n",
    "# PRETRAINED_MODEL='/home/aa2650/playground/ECG-FM/ckpts/mimic_iv_ecg_physionet_pretrained.pt'\n",
    "# MANIFEST_DIR=f\"/home/aa2650/datasets/code_15/subset/manifests/\"\n",
    "# LABEL_DIR=\"/home/aa2650/datasets/code_15/subset\"\n",
    "# OUTPUT_DIR=f'/home/aa2650/playground/ECG-FM/experiments/subset/{experiment_run}'\n",
    "# NUM_LABELS=6\n",
    "# # NUM_LABELS=$(($(wc -l < \"$/home/aa2650/playground/ECG-FM/data/code_15/labels/label_def.csv\") - 1))\n",
    "# # POS_WEIGHT=$(cat $LABEL_DIR/pos_weight.txt)\n",
    "\n",
    "# # checkpoint.save_dir\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched training under nohup → logs at /home/aa2650/playground/ECG-FM/experiments/raw/5/train.log\n"
     ]
    }
   ],
   "source": [
    "train = 1\n",
    "validation = 1\n",
    "test = 98\n",
    "data_split = f\"{train}-{validation}-{test}\"\n",
    "experiment_run = 5\n",
    "\n",
    "PRETRAINED_MODEL='/home/aa2650/playground/ECG-FM/ckpts/mimic_iv_ecg_physionet_pretrained.pt'\n",
    "MANIFEST_DIR=f\"/home/aa2650/datasets/code_15/subset/manifests/\"\n",
    "LABEL_DIR=\"/home/aa2650/datasets/code_15/subset/1-1-98\"\n",
    "OUTPUT_DIR=f'/home/aa2650/playground/ECG-FM/experiments/raw/{experiment_run}'\n",
    "NUM_LABELS=6\n",
    "# NUM_LABELS=$(($(wc -l < \"$/home/aa2650/playground/ECG-FM/data/code_15/labels/label_def.csv\") - 1))\n",
    "# POS_WEIGHT=$(cat $LABEL_DIR/pos_weight.txt)\n",
    "\n",
    "# checkpoint.save_dir\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# build the “finetune” string exactly as before…\n",
    "finetune_cmd = f\"\"\"export HYDRA_FULL_ERROR=1 && \\\n",
    "fairseq-hydra-train \\\n",
    "    common.seed=5348679 \\\n",
    "    task.data={MANIFEST_DIR} \\\n",
    "    model.model_path={PRETRAINED_MODEL} \\\n",
    "    model.num_labels={NUM_LABELS} \\\n",
    "    optimization.lr=[1e-12] \\\n",
    "    optimization.max_epoch=1 \\\n",
    "    dataset.batch_size=32 \\\n",
    "    dataset.num_workers=5 \\\n",
    "    dataset.disable_validation=true \\\n",
    "    distributed_training.distributed_world_size=1 \\\n",
    "    distributed_training.find_unused_parameters=True \\\n",
    "    checkpoint.save_dir={OUTPUT_DIR} \\\n",
    "    checkpoint.save_interval=1 \\\n",
    "    checkpoint.keep_last_epochs=0 \\\n",
    "    common.log_format=csv \\\n",
    "    common.memory_efficient_fp16=True \\\n",
    "    +task.label_file=/home/aa2650/datasets/code_15/subset/subset_y.npy \\\n",
    "    --config-dir {FAIRSEQ_SIGNALS_ROOT}/examples/w2v_cmsc/config/finetuning/ecg_transformer \\\n",
    "    --config-name diagnosis\n",
    "\"\"\"\n",
    "\n",
    "# wrap with nohup → write both stdout+stderr into train.log\n",
    "nohup_cmd = f\"nohup bash -lc \\\"{finetune_cmd}\\\" > {OUTPUT_DIR}/train.log 2>&1 &\"\n",
    "\n",
    "# launch it\n",
    "os.system(nohup_cmd)\n",
    "print(f\"Launched training under nohup → logs at {OUTPUT_DIR}/train.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the “finetune” string exactly as before…\n",
    "# finetune_cmd = f\"\"\"export HYDRA_FULL_ERROR=1 && \\\n",
    "# fairseq-hydra-train \\\n",
    "#     common.seed=55 \\\n",
    "#     task.data={MANIFEST_DIR} \\\n",
    "#     model.model_path={PRETRAINED_MODEL} \\\n",
    "#     model.num_labels={NUM_LABELS} \\\n",
    "#     optimization.lr=[1e-06] \\\n",
    "#     optimization.max_epoch=25 \\\n",
    "#     dataset.batch_size=64 \\\n",
    "#     dataset.num_workers=5 \\\n",
    "#     dataset.disable_validation=true \\\n",
    "#     distributed_training.distributed_world_size=1 \\\n",
    "#     distributed_training.find_unused_parameters=True \\\n",
    "#     checkpoint.save_dir={OUTPUT_DIR} \\\n",
    "#     checkpoint.save_interval=1 \\\n",
    "#     checkpoint.keep_last_epochs=0 \\\n",
    "#     common.log_format=csv \\\n",
    "#     common.memory_efficient_fp16=True \\\n",
    "#     +task.label_file=/home/aa2650/datasets/code_15/subset/subset_y.npy \\\n",
    "#     --config-dir {FAIRSEQ_SIGNALS_ROOT}/examples/w2v_cmsc/config/finetuning/ecg_transformer \\\n",
    "#     --config-name diagnosis\n",
    "# \"\"\"\n",
    "\n",
    "# # wrap with nohup → write both stdout+stderr into train.log\n",
    "# nohup_cmd = f\"nohup bash -lc \\\"{finetune_cmd}\\\" > {OUTPUT_DIR}/train.log 2>&1 &\"\n",
    "\n",
    "# # launch it\n",
    "# os.system(nohup_cmd)\n",
    "# print(f\"Launched training under nohup → logs at {OUTPUT_DIR}/train.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old command for fine tuning that theoretically should work\n",
    "\n",
    "# train = 1\n",
    "# validation = 1\n",
    "# test = 98\n",
    "# data_split = f\"{train}-{validation}-{test}\"\n",
    "\n",
    "# PRETRAINED_MODEL='/home/aa2650/playground/ECG-FM/ckpts/mimic_iv_ecg_physionet_pretrained.pt'\n",
    "# MANIFEST_DIR=f\"/home/aa2650/datasets/code_15/manifests/{data_split}\"\n",
    "# LABEL_DIR=\"/home/aa2650/datasets/code_15\"\n",
    "# OUTPUT_DIR=f'/home/aa2650/playground/ECG-FM/experiments/{data_split}'\n",
    "# NUM_LABELS=8\n",
    "# # NUM_LABELS=$(($(wc -l < \"$/home/aa2650/playground/ECG-FM/data/code_15/labels/label_def.csv\") - 1))\n",
    "# # POS_WEIGHT=$(cat $LABEL_DIR/pos_weight.txt)\n",
    "\n",
    "# finetune_cmd = f\"\"\"fairseq-hydra-train \\\n",
    "#     task.data={MANIFEST_DIR} \\\n",
    "#     model.model_path={PRETRAINED_MODEL} \\\n",
    "#     model.num_labels={NUM_LABELS} \\\n",
    "#     optimization.lr=[1e-06] \\\n",
    "#     optimization.max_epoch=140 \\\n",
    "#     dataset.batch_size=32 \\\n",
    "#     dataset.num_workers=5 \\\n",
    "#     dataset.disable_validation=true \\\n",
    "#     distributed_training.distributed_world_size=1 \\\n",
    "#     distributed_training.find_unused_parameters=True \\\n",
    "#     checkpoint.save_dir={OUTPUT_DIR} \\\n",
    "#     checkpoint.save_interval=1 \\\n",
    "#     checkpoint.keep_last_epochs=0 \\\n",
    "#     common.memory_efficient_fp16=True \\\n",
    "#     common.log_format=csv \\\n",
    "#     +task.label_file={LABEL_DIR}/y.npy \\\n",
    "#     --config-dir {FAIRSEQ_SIGNALS_ROOT}/examples/w2v_cmsc/config/finetuning/ecg_transformer \\\n",
    "#     --config-name diagnosis\n",
    "# \"\"\"\n",
    "# os.system(finetune_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"/home/aa2650/datasets/code_15/labels.csv\" \n",
    "# output_path = \"/home/aa2650/datasets/code_15/y.npy\"\n",
    "\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Extract only the label columns (drop 'idx')\n",
    "# label_columns = df.columns[1:]\n",
    "# labels_df = df[label_columns]\n",
    "\n",
    "# y_array = labels_df.astype(np.float64).to_numpy()\n",
    "\n",
    "# np.save(output_path, y_array)\n",
    "\n",
    "# print(f\"Saved y.npy with shape {y_array.shape} and dtype {y_array.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# pos_output_path = \"/home/aa2650/datasets/code_15/labels_def.csv\"\n",
    "\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# label_cols = df.columns[2:]\n",
    "# labels_only = df[label_cols]\n",
    "\n",
    "# pos_count_all = labels_only.sum()\n",
    "# pos_percent_all = labels_only.mean()\n",
    "\n",
    "# label_def_new = pd.DataFrame({\n",
    "#     \"name\": label_cols,\n",
    "#     \"pos_count_all\": pos_count_all.values,\n",
    "#     \"pos_percent_all\": pos_percent_all.values\n",
    "# })\n",
    "\n",
    "# label_def_new.to_csv(pos_output_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
